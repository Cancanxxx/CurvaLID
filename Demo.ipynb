{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Add your datasets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "from datasets import load_dataset\n",
    "import json\n",
    "import os\n",
    "import csv\n",
    "\n",
    "# import your data from csv/ txt files...\n",
    "# Each dataset shall be imported as a list of strings\n",
    "# e.g. SAP_input = [\"hello world\", \"I am here\", \"I think you get the idea\", \"u do get the idea\"]\n",
    "\n",
    "print(f\"SAP_input: {len(SAP_input)}\")\n",
    "print(f\"DAN: {len(DAN)}\")\n",
    "print(f\"MWP: {len(MWP)}\")\n",
    "print(f\"GCG_prompts: {len(GCG_prompts)}\")\n",
    "\n",
    "print(f\"b_input_Orca: {len(b_input_Orca)}\")\n",
    "print(f\"b_input_mmlu: {len(b_input_mmlu)}\")\n",
    "print(f\"b_input_alpEval: {len(b_input_alpEval)}\")\n",
    "print(f\"b_input_TQA: {len(b_input_TQA)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get the word embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import RobertaTokenizer, RobertaModel\n",
    "import numpy as np\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
    "roberta_model = RobertaModel.from_pretrained('roberta-base')\n",
    "\n",
    "def split_text(text, max_length=512):\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    return [' '.join(tokens[i:i + max_length]) for i in range(0, len(tokens), max_length)]\n",
    "\n",
    "def get_roberta_embeddings(text_chunks):\n",
    "    embeddings = []\n",
    "    for chunk in text_chunks:\n",
    "        inputs = tokenizer(chunk, return_tensors='pt', truncation=True, max_length=512)\n",
    "        with torch.no_grad():\n",
    "            outputs = roberta_model(**inputs)\n",
    "        embeddings.append(outputs.last_hidden_state.squeeze().numpy())\n",
    "    return np.vstack(embeddings)\n",
    "\n",
    "def process_text(text):\n",
    "    return get_roberta_embeddings(split_text(text))\n",
    "\n",
    "def generate_embeddings_parallel(texts, max_workers=4):\n",
    "    embeddings = [None] * len(texts)  # Initialize list with None to preserve order\n",
    "    with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        futures = {executor.submit(process_text, text): idx for idx, text in enumerate(texts)}\n",
    "        for future in tqdm(as_completed(futures), total=len(texts), desc=\"Generating Embeddings\"):\n",
    "            idx = futures[future]\n",
    "            embeddings[idx] = future.result()\n",
    "    return embeddings\n",
    "\n",
    "embeddings_benign_Orca = generate_embeddings_parallel(b_input_Orca)\n",
    "embeddings_benign_mmlu = generate_embeddings_parallel(b_input_mmlu)\n",
    "embeddings_benign_alphEval = generate_embeddings_parallel(b_input_alpEval)\n",
    "embeddings_benign_TQA = generate_embeddings_parallel(b_input_TQA)\n",
    "\n",
    "embeddings_adversarial_SAP = generate_embeddings_parallel(SAP_input)\n",
    "embeddings_adversarial_DAN = generate_embeddings_parallel(DAN)\n",
    "embeddings_adversarial_MWP = generate_embeddings_parallel(MWP)\n",
    "embeddings_adversarial_GCG = generate_embeddings_parallel(GCG_prompts)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pad the prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = embeddings_benign_Orca+embeddings_benign_mmlu+embeddings_benign_alphEval+embeddings_adversarial_SAP+embeddings_adversarial_DAN+embeddings_adversarial_MWP+embeddings_adversarial_GCG+embeddings_benign_TQA)\n",
    "max_length = max(len(text) for text in texts)\n",
    "embedding_dim = 768\n",
    "\n",
    "def pad_to_max_length(arrays, max_length, pad_length=768):\n",
    "    padded_arrays = []\n",
    "    for array in arrays:\n",
    "        padding_needed = max_length - array.shape[0]\n",
    "        if padding_needed > 0:\n",
    "            padding = np.zeros((padding_needed, pad_length))\n",
    "            padded_array = np.vstack((array, padding))\n",
    "        else:\n",
    "            padded_array = array\n",
    "        padded_arrays.append(padded_array)\n",
    "    return np.array(padded_arrays)\n",
    "\n",
    "embeddings_benign_Orca = pad_to_max_length(embeddings_benign_Orca, max_length)\n",
    "embeddings_benign_mmlu = pad_to_max_length(embeddings_benign_mmlu, max_length)\n",
    "embeddings_benign_alphEval = pad_to_max_length(embeddings_benign_alphEval, max_length)\n",
    "embeddings_benign_TQA = pad_to_max_length(embeddings_benign_TQA, max_length)\n",
    "embeddings_adversarial_SAP = pad_to_max_length(embeddings_adversarial_SAP, max_length)\n",
    "embeddings_adversarial_DAN = pad_to_max_length(embeddings_adversarial_DAN, max_length)\n",
    "embeddings_adversarial_MWP = pad_to_max_length(embeddings_adversarial_MWP, max_length)\n",
    "embeddings_adversarial_GCG = pad_to_max_length(embeddings_adversarial_GCG, max_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Standardize the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "def standardize_data(padded_data):\n",
    "    original_shape = padded_data.shape\n",
    "    flat_data = padded_data.reshape(-1, original_shape[-1])\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    standardized_flat_data = scaler.fit_transform(flat_data)\n",
    "\n",
    "    standardized_data = standardized_flat_data.reshape(original_shape)\n",
    "    return standardized_data\n",
    "\n",
    "embeddings_benign_Orca = standardize_data(embeddings_benign_Orca)\n",
    "embeddings_benign_mmlu = standardize_data(embeddings_benign_mmlu)\n",
    "embeddings_benign_alphEval = standardize_data(embeddings_benign_alphEval)\n",
    "embeddings_benign_TQA = standardize_data(embeddings_benign_TQA)\n",
    "embeddings_adversarial_SAP = standardize_data(embeddings_adversarial_SAP)\n",
    "embeddings_adversarial_DAN = standardize_data(embeddings_adversarial_DAN)\n",
    "embeddings_adversarial_MWP = standardize_data(embeddings_adversarial_MWP)\n",
    "embeddings_adversarial_GCG = standardize_data(embeddings_adversarial_GCG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1 of CurvaLID: Train the CNN for classifying benign datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Dense, Conv1D, Flatten, Input\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "labels_benign_Orca = np.zeros(len(embeddings_benign_Orca), dtype=int)\n",
    "labels_benign_mmlu = np.ones(len(embeddings_benign_mmlu), dtype=int)\n",
    "labels_benign_alphEval = np.full(len(embeddings_benign_alphEval), 2, dtype=int)\n",
    "labels_benign_TQA = np.full(len(embeddings_benign_TQA), 3, dtype=int)\n",
    "\n",
    "X = np.concatenate([embeddings_benign_Orca, embeddings_benign_mmlu, embeddings_benign_alphEval, embeddings_benign_TQA], axis=0)\n",
    "y = np.concatenate([labels_benign_Orca, labels_benign_mmlu, labels_benign_alphEval, labels_benign_TQA], axis=0)\n",
    "\n",
    "encoder = OneHotEncoder(sparse_output=False)\n",
    "y_one_hot = encoder.fit_transform(y.reshape(-1, 1))\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y_one_hot, test_size=0.2)\n",
    "\n",
    "input_shape = X_train.shape[1:]\n",
    "inputs = Input(shape=input_shape)\n",
    "x = Conv1D(32, kernel_size=3, activation='relu')(inputs)\n",
    "x = Conv1D(64, kernel_size=3, activation='relu')(x)\n",
    "x = Flatten()(x)\n",
    "x = Dense(128, activation='relu')(x)\n",
    "outputs = Dense(4, activation='softmax')(x)\n",
    "\n",
    "model = Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "model.fit(X_train, y_train, epochs=20, batch_size=32, validation_split=0.2)\n",
    "\n",
    "model.save('trained_model_2.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get more layer output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_layer_outputs(model, data):\n",
    "    layer_outputs = [layer.output for layer in model.layers]\n",
    "    activation_model = Model(inputs=model.input, outputs=layer_outputs)\n",
    "    return activation_model.predict(data)\n",
    "\n",
    "activations = get_layer_outputs(model, X_test)\n",
    "\n",
    "for layer_activation in activations:\n",
    "    print(layer_activation.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get LID on layer 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Model\n",
    "from scipy.stats import kurtosis\n",
    "from tqdm import tqdm\n",
    "from joblib import Parallel, delayed\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "txt_ben = np.concatenate([embeddings_benign_Orca, embeddings_benign_mmlu, embeddings_benign_alphEval, embeddings_benign_TQA], axis=0)\n",
    "txt_adv = np.concatenate([embeddings_adversarial_SAP, embeddings_adversarial_DAN, embeddings_adversarial_MWP, embeddings_adversarial_GCG], axis=0)\n",
    "\n",
    "def get_specific_layer_outputs(model, data, layer_indices):\n",
    "    layer_outputs = [model.layers[i].output for i in layer_indices]\n",
    "    activation_model = Model(inputs=model.input, outputs=layer_outputs)\n",
    "    return activation_model.predict(data)\n",
    "\n",
    "layer_indices = [4]  \n",
    "\n",
    "activations_benign = get_specific_layer_outputs(model, txt_ben, layer_indices)\n",
    "activations_adversarial = get_specific_layer_outputs(model, txt_adv, layer_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "def lid_mom_est_minibatch(data, reference, k, batch_size, compute_mode='use_mm_for_euclid_dist_if_necessary'):\n",
    "    lids = []\n",
    "    num_batches = int(np.ceil(len(data) / batch_size))\n",
    "    \n",
    "    for i in tqdm(range(num_batches), desc=\"Calculating LID in minibatches\"):\n",
    "        start_idx = i * batch_size\n",
    "        end_idx = min((i + 1) * batch_size, len(data))\n",
    "        \n",
    "        batch_data = torch.tensor(data[start_idx:end_idx], dtype=torch.float32)\n",
    "        batch_reference = torch.tensor(reference, dtype=torch.float32)\n",
    "        \n",
    "        b = batch_data.shape[0]\n",
    "        k = min(k, b - 2)\n",
    "        \n",
    "        # Calculate the pairwise distances using PyTorch\n",
    "        r = torch.cdist(batch_data, batch_reference, p=2, compute_mode=compute_mode)\n",
    "        a, _ = torch.sort(r, dim=1)\n",
    "        \n",
    "        # Mean distance to k nearest neighbors\n",
    "        m = torch.mean(a[:, 1:k], dim=1)\n",
    "        \n",
    "        # Method of Moments estimation of LID\n",
    "        batch_lids = m / (a[:, k] - m)\n",
    "        lids.append(batch_lids)\n",
    "    \n",
    "    return torch.cat(lids, dim=0)\n",
    "\n",
    "k = 32\n",
    "batch_size = 32\n",
    "\n",
    "lid_benign = lid_mom_est_minibatch(activations_benign, activations_benign, k, batch_size)\n",
    "\n",
    "lid_adversarial = lid_mom_est_minibatch(activations_adversarial, activations_benign, k, batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Curvature on layer 1 and 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Model\n",
    "from scipy.stats import kurtosis\n",
    "from tqdm import tqdm\n",
    "from joblib import Parallel, delayed\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "txt_ben = np.concatenate([embeddings_benign_Orca, embeddings_benign_mmlu, embeddings_benign_alphEval, embeddings_benign_TQA], axis=0)\n",
    "txt_adv = np.concatenate([embeddings_adversarial_SAP, embeddings_adversarial_DAN, embeddings_adversarial_MWP, embeddings_adversarial_GCG], axis=0)\n",
    "\n",
    "def get_specific_layer_outputs(model, data, layer_indices):\n",
    "    layer_outputs = [model.layers[i].output for i in layer_indices]\n",
    "    activation_model = Model(inputs=model.input, outputs=layer_outputs)\n",
    "    return activation_model.predict(data)\n",
    "\n",
    "layer_indices = [1]\n",
    "\n",
    "activations_benign = get_specific_layer_outputs(model, txt_ben, layer_indices)\n",
    "activations_adversarial = get_specific_layer_outputs(model, txt_adv, layer_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def calculate_curvature(embeddings):\n",
    "    curvatures = []\n",
    "    for i in range(1, len(embeddings)):\n",
    "        p0 = embeddings[i - 1]\n",
    "        p1 = embeddings[i]\n",
    "        norm_p0 = np.linalg.norm(p0)\n",
    "        norm_p1 = np.linalg.norm(p1)\n",
    "        if norm_p0 > 0 and norm_p1 > 0:\n",
    "            cosine_angle = np.dot(p0, p1) / (norm_p0 * norm_p1)\n",
    "            angular_change = np.arccos(np.clip(cosine_angle, -1.0, 1.0))\n",
    "            distance_change = 1/norm_p0 + 1/norm_p1\n",
    "            curvature = angular_change / distance_change\n",
    "            curvatures.append(curvature)\n",
    "    return np.mean(curvatures) if curvatures else 0\n",
    "\n",
    "def calculate_mean_curvature(activations):\n",
    "    mean_curvatures = []\n",
    "    for activation in activations:\n",
    "        mean_curvature = calculate_curvature(activation)\n",
    "        mean_curvatures.append(mean_curvature)\n",
    "    return np.array(mean_curvatures)\n",
    "\n",
    "mean_curvatures_benign = calculate_mean_curvature(activations_benign)\n",
    "mean_curvatures_adversarial = calculate_mean_curvature(activations_adversarial)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "overall_mean_curvature_benign = np.mean(mean_curvatures_benign)\n",
    "overall_mean_curvature_adversarial = np.mean(mean_curvatures_adversarial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Model\n",
    "from scipy.stats import kurtosis\n",
    "from tqdm import tqdm\n",
    "from joblib import Parallel, delayed\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "txt_ben = np.concatenate([embeddings_benign_Orca, embeddings_benign_mmlu, embeddings_benign_alphEval, embeddings_benign_TQA], axis=0)\n",
    "txt_adv = np.concatenate([embeddings_adversarial_SAP, embeddings_adversarial_DAN, embeddings_adversarial_MWP, embeddings_adversarial_GCG], axis=0)\n",
    "\n",
    "\n",
    "def get_specific_layer_outputs(model, data, layer_indices):\n",
    "    layer_outputs = [model.layers[i].output for i in layer_indices]\n",
    "    activation_model = Model(inputs=model.input, outputs=layer_outputs)\n",
    "    return activation_model.predict(data)\n",
    "\n",
    "layer_indices = [2] \n",
    "\n",
    "activations_benign = get_specific_layer_outputs(model, txt_ben, layer_indices)\n",
    "activations_adversarial = get_specific_layer_outputs(model, txt_adv, layer_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_mean_curvature(activations):\n",
    "    mean_curvatures = []\n",
    "    for activation in activations:\n",
    "        mean_curvature = calculate_curvature(activation)\n",
    "        mean_curvatures.append(mean_curvature)\n",
    "    return np.array(mean_curvatures)\n",
    "\n",
    "mean_curvatures_benign2 = calculate_mean_curvature(activations_benign)\n",
    "mean_curvatures_adversarial2 = calculate_mean_curvature(activations_adversarial)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "overall_mean_curvature_benign = np.mean(mean_curvatures_benign2)\n",
    "overall_mean_curvature_adversarial = np.mean(mean_curvatures_adversarial2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_benign = np.column_stack((lid_benign, mean_curvatures_benign, mean_curvatures_benign2))\n",
    "\n",
    "features_adversarial = np.column_stack((lid_adversarial, mean_curvatures_adversarial, mean_curvatures_adversarial2))\n",
    "\n",
    "features = np.vstack((features_benign, features_adversarial))\n",
    "labels_benign = np.zeros(features_benign.shape[0])\n",
    "labels_adversarial = np.ones(features_adversarial.shape[0])\n",
    "labels = np.concatenate([labels_benign, labels_adversarial])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Normalize the features\n",
    "scaler = StandardScaler()\n",
    "features_normalized = scaler.fit_transform(features)\n",
    "\n",
    "# Define dataset identifiers\n",
    "dataset_identifiers = np.concatenate([\n",
    "    np.full(len(embeddings_benign_Orca), 0),\n",
    "    np.full(len(embeddings_benign_mmlu), 1),\n",
    "    np.full(len(embeddings_benign_alphEval), 2),\n",
    "    np.full(len(embeddings_benign_TQA), 3),\n",
    "    np.full(len(embeddings_adversarial_SAP), 4),\n",
    "    np.full(len(embeddings_adversarial_DAN), 5),\n",
    "    np.full(len(embeddings_adversarial_MWP), 6),\n",
    "    np.full(len(embeddings_adversarial_GCG), 7)\n",
    "])\n",
    "\n",
    "# Function to create the improved MLP model\n",
    "def create_improved_model(optimizer='adam', learning_rate=0.001, neurons=256, dropout_rate=0.5, activation='relu'):\n",
    "    model = Sequential([\n",
    "        Dense(neurons, activation=activation, input_shape=(features_normalized.shape[1],), kernel_regularizer='l2'),\n",
    "        BatchNormalization(),\n",
    "        Dropout(dropout_rate),\n",
    "        Dense(neurons // 2, activation=activation, kernel_regularizer='l2'),\n",
    "        BatchNormalization(),\n",
    "        Dropout(dropout_rate),\n",
    "        Dense(2, activation='softmax')  # 2 output units for benign and adversarial classes\n",
    "    ])\n",
    "    \n",
    "    opt = optimizer(learning_rate=learning_rate)\n",
    "    model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test, train_ids, test_ids = train_test_split(\n",
    "    features_normalized, labels, dataset_identifiers, test_size=0.2\n",
    ")\n",
    "\n",
    "# One-hot encode the labels for the classifier\n",
    "y_train_one_hot = to_categorical(y_train)\n",
    "y_test_one_hot = to_categorical(y_test)\n",
    "\n",
    "# Create the model with improved parameters\n",
    "model = create_improved_model(optimizer=Adam, learning_rate=0.001, neurons=256, dropout_rate=0.5, activation='relu')\n",
    "\n",
    "# Early stopping to prevent overfitting\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "\n",
    "# Train the classifier\n",
    "model.fit(X_train, y_train_one_hot, epochs=150, batch_size=64, validation_split=0.2, callbacks=[early_stopping], verbose=0)\n",
    "\n",
    "# Evaluate the classifier\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "accuracy_mlp = accuracy_score(y_test, y_pred_classes)\n",
    "print(f\"Overall Accuracy: {accuracy_mlp}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
